---
title: "Kernel Regression Analysis"
author: "Kyle Del Balso, Zack Starrett, Swornim Bhandari"
format: revealjs
editor: visual
---

## Introduction

Data that assumes a non-linear relationship can be handled through non-parametric regression, which will help accurately find the conditional expectation between two random variables. Kernel regression is a popular form of non-parametric regression. Specifically, Kernel regression uses a kernel smoothing method to estimate a curve for non-parametric data.

## Introduction

Kernel smoothing is done by taking the weighted average of training samples that surround a targeted y value. Inside this smoothing parameter is distance-based kernel function, which helps attach weights to the observations surrounding the y value. The Nadaraya-Watson Kernel Estimator, which was founded in 1964, has been considered a very effective approach in accurately modeling real world data.

## Introduction

Support Vector Machines (SVM) is a type of supervised machine learning method that has been shown to have better performance measurements than other machine learning methods such as the Decision Tree and Neural Network models. SVMs solve binary classification problems by defining a maximum margin separating hyperplane. This hyperplane is then used to classify training and testing datapoints. To construct the best hyperplane, SVMs need support vectors, or datapoints around the boundary. For non-linear data, kernel SVMs allow the hyperplane to be mapped on a higher dimensional space(Awad).

## Introduction

This report presents a two-part study. The first section addresses how to determine the optimal parameters for Nadaryan-Watson Kernel estimator. We will examine the bias-variance tradeoff and conclude the optimal bandwidth using a commonly used cars dataset in R. The second section focuses on kernel SVM, answering questions about how to fine tune the regularization and sensitivity parameters discussed above. We will conclude with a comparative analysis of the four different kernel functions provided in R.

## Methods (Kernel Estimator)

The Nadarayan-Watson kernel estimator is an important nonparametric kernel estimator of a regression function which is achieved by using a fixed bandwidth. The NW method is a method for estimating unknown parameters of a regression function and is suitable for the situation where the data comes from a joint probability distribution function, f (x, y). The non- parametric regression model for NW method is

![](https://basset10.github.io/KernelRegressionProject/NWNP1.png){fig-align="center" width="250" height="65"}

![](https://basset10.github.io/KernelRegressionProject/NWNP2.png){fig-align="center" width="250"}

## Methods (Kernel SVM)

The idea of SVM was established in 1958 by Rosenblatt. SVM is one of the popular and efficient classification and regression methods which separates hyperplane with maximum margin in two-dimensional space which are linearly separable. The one that maximizes the distance to the closest data points from both classes is called hyperplane with maximum margin.

## Methods (Kernel SVM)

A Kernel trick is a method where a non-linear data is projected onto a higher dimension space to make it easier to classify the data where it could be linearly divided by a plane (Karatzoglou, Meyer, & Hornik, 2006). SVMs use an implicit mapping Î¦ of the input data into a high-dimensional feature space. The common classification model is

![](https://basset10.github.io/KernelRegressionProject/svm.png){width="413" height="62"}

Where b is the bias of the model.


## Methods (Kernel SVM)

Solving the subsequent constrained optimization and creating decision boundary, we get

![](https://basset10.github.io/KernelRegressionProject/svm2.png){fig-align="center" width="399"}
As it is constrained optimization problem it needs Lagrangian formula or Lagrangian multipliers.

## Methods (Kernel SVM)

Firstly, we need to find decision boundary for linearly separable plane and then solve with Lagrange multipliers. We get following equation,

![](https://basset10.github.io/KernelRegressionProject/svm4.png){fig-align="center" width="546"}

## Types of Kernel Functions

There are five main types of kernels, listed here with equations:

Linear Kernel Function

![](https://basset10.github.io/KernelRegressionProject/linearkernel.png){width="292"}

Homogenous Kernel Function

![](https://basset10.github.io/KernelRegressionProject/Homogenous.png){width="292"}

## Types of Kernel Functions, cont.

Inhomogenous Kernel Function

![](https://basset10.github.io/KernelRegressionProject/inhomogenous.png){width="284"}

Gaussian RBF Kernel Function

![](https://basset10.github.io/KernelRegressionProject/Gaussian.png){width="352"}

Sigmoid Kernel Function

![](https://basset10.github.io/KernelRegressionProject/Sigmoid.png){width="373"}

## Analysis and Results

Attempting to obtain the best estimated curve for Nadarayan-Watson Kernel estimator requires an input of the bandwidth h. A higher bandwidth will include more data points for the weighted average, while still giving the highest weights to the closer data points. Choosing a bandwidth is vital in finding a function estimate that fits the observed sample of data while providing a smooth function estimate (Helwig).

## Analysis and Results

Too small a bandwidth, and the estimated curve will overfit the data, which will negatively affect the accuracy of predicting future observations. Too large a bandwidth, and the estimated curve will underfit the data, which results in the estimated curve missing the true relationship between the input values. The question to analyze becomes clear; How do we optimize the bandwidth, so our estimated curve minimizes the negative tradeoff effects?

## Analysis and Results

We also wish to examine the impact of kernels for classification, specifically kernel SVM models. We will use R programming language to design four kernel SVM models on the Algerian Forest Fire dataset: linear, polynomial, radial basis function, and sigmoid. We will discover the optimal parameter for each kernel using a tuning parameter in R. Once we obtain the best models for each kernel, we will run a comparative analysis to determine which is superior in its accuracy on the testing set.

## Datasets for Experimentation

-   *Cars* dataset provides a 1920s dataset containing 50 observations. There are two variables in the dataset: the X variable is the speed of cars, and the Y variable is the distance taken to stop(Ezekiel).

-   *Algerian Forest Fires* Dataset for the Bajaia region located in the northeast of Algeria, which includes 122 observations. The dataset has a binary response variable that indicates whether the Bajai region of Algeria experienced no fire = 0, or a fire = 1. The dataset contains 10 explanatory variables including temperature, relative humidity, wind speed, rain, fine fuel moisture code, duff moisture code, drought code, initial spread index, and fire weather index (Dua).

## Results

For the Algerian Forest Fires Dataset, we will split the data into training and testing sets. Specifically, 75% of the data will go into the training set, while the remaining 25% will be used for testing the accuracy of the model.

## Results

Using the cars dataset in R, we obtain four plots in Figure 4.1. Each regression line is obtained using the ksmooth() function. The top left figure is a scatter plot between the predictor variable speed and the response variable stopping distance. The data appears to be normal when speed \< 15. However, the data when speed \> 15 appears to have a non-linear relationship with stopping distance. 

## Results

Since the data illustrates a non-linear relationship, nonparametric kernel regression is used to determine the regression curve. The figure on the top right shows the regression curve estimation with a bandwidth = 20. Although this result produces a stable line with little variance, the bias tradeoff is too high as the line does not fit the data well. The figure on the bottom left shows the regression curve estimation with a bandwidth = 1. The line is too rigid and overfits the data.



## Results

![](https://basset10.github.io/KernelRegressionProject/bandwidth.png)

Figure 1: The graph of the data with different bandwidths(Helwig)

## Results

The optimal bandwidth is obtained through generalized cross-validation using the np.gcv() function with a selected \"gaussian\" kernel (Kauermann). The generalized cross-validation procedure gives us an optimal bandwidth value of 5.928571. The bottom right figure shows the regression curve estimation with the obtained optimal bandwidth = 5.928571. The line now is smooth and fits the data appropriately. Figure 4.2 shows the three chosen bandwidths together.



## Results

![](https://basset10.github.io/KernelRegressionProject/bandwidth2.png)

Figure 4.2: The combined graph of the different bandwidths(Helwig)

## Results

The experiment on real world data is a fruitful process to analyze the impact the effect of choosing an appropriate bandwidth. In the car\'s dataset, we have no assumptions about the data, so we needed to figure out a bandwidth that optimizes the variance-bias tradeoff. Too small a bandwidth and we have too much variance. Too large a bandwidth and we don\'t cover all the data. Therefore, finding the optimal bandwidth is crucial in determining the best regression curve.

## Results

Even though R uses a default bandwidth of h = 0.5, that does not indicate that the optimal bandwidth is so. There is no uniform way to obtain this metric, and in our experiment, we used a cross-validation technique to approximate the best regression curve(Kauermann).
